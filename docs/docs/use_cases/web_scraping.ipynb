{
  "cells": [
    {
      "cell_type": "raw",
      "id": "e254cf03-49fc-4051-a4df-3a8e4e7d2688",
      "metadata": {
        "id": "e254cf03-49fc-4051-a4df-3a8e4e7d2688"
      },
      "source": [
        "---\n",
        "title: Web scraping\n",
        "sidebar_class_name: hidden\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6605e7f7",
      "metadata": {
        "id": "6605e7f7"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/v0.1/docs/docs/use_cases/web_scraping.ipynb)\n",
        "\n",
        "## Use case\n",
        "\n",
        "[Web research](https://blog.langchain.dev/automating-web-research/) is one of the killer LLM applications:\n",
        "\n",
        "* Users have [highlighted it](https://twitter.com/GregKamradt/status/1679913813297225729?s=20) as one of his top desired AI tools.\n",
        "* OSS repos like [gpt-researcher](https://github.com/assafelovic/gpt-researcher) are growing in popularity.\n",
        "\n",
        "![Image description](https://github.com/langchain-ai/langchain/blob/v0.1/docs/static/img/web_scraping.png?raw=1)\n",
        "\n",
        "## Overview\n",
        "\n",
        "Gathering content from the web has a few components:\n",
        "\n",
        "* `Search`: Query to url (e.g., using `GoogleSearchAPIWrapper`).\n",
        "* `Loading`: Url to HTML  (e.g., using `AsyncHtmlLoader`, `AsyncChromiumLoader`, etc).\n",
        "* `Transforming`: HTML to formatted text (e.g., using `HTML2Text` or `Beautiful Soup`).\n",
        "\n",
        "## Quickstart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1803c182",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1803c182",
        "outputId": "b228df1f-5f46-481d-d2f6-5a757e80a8f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Chromium 130.0.6723.31 (playwright build v1140)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1140/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G164.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 0% 6.5s\u001b[0K\u001b[1G164.5 MiB [] 0% 5.5s\u001b[0K\u001b[1G164.5 MiB [] 1% 3.6s\u001b[0K\u001b[1G164.5 MiB [] 1% 3.2s\u001b[0K\u001b[1G164.5 MiB [] 2% 3.0s\u001b[0K\u001b[1G164.5 MiB [] 3% 3.1s\u001b[0K\u001b[1G164.5 MiB [] 4% 3.1s\u001b[0K\u001b[1G164.5 MiB [] 4% 3.2s\u001b[0K\u001b[1G164.5 MiB [] 5% 3.2s\u001b[0K\u001b[1G164.5 MiB [] 5% 3.1s\u001b[0K\u001b[1G164.5 MiB [] 5% 3.2s\u001b[0K\u001b[1G164.5 MiB [] 6% 3.0s\u001b[0K\u001b[1G164.5 MiB [] 7% 2.9s\u001b[0K\u001b[1G164.5 MiB [] 8% 2.7s\u001b[0K\u001b[1G164.5 MiB [] 9% 2.7s\u001b[0K\u001b[1G164.5 MiB [] 9% 2.6s\u001b[0K\u001b[1G164.5 MiB [] 10% 2.6s\u001b[0K\u001b[1G164.5 MiB [] 11% 2.6s\u001b[0K\u001b[1G164.5 MiB [] 12% 2.5s\u001b[0K\u001b[1G164.5 MiB [] 13% 2.4s\u001b[0K\u001b[1G164.5 MiB [] 14% 2.4s\u001b[0K\u001b[1G164.5 MiB [] 15% 2.3s\u001b[0K\u001b[1G164.5 MiB [] 16% 2.3s\u001b[0K\u001b[1G164.5 MiB [] 17% 2.2s\u001b[0K\u001b[1G164.5 MiB [] 17% 2.1s\u001b[0K\u001b[1G164.5 MiB [] 18% 2.1s\u001b[0K\u001b[1G164.5 MiB [] 19% 2.0s\u001b[0K\u001b[1G164.5 MiB [] 20% 2.0s\u001b[0K\u001b[1G164.5 MiB [] 21% 2.0s\u001b[0K\u001b[1G164.5 MiB [] 22% 1.9s\u001b[0K\u001b[1G164.5 MiB [] 23% 1.9s\u001b[0K\u001b[1G164.5 MiB [] 24% 1.9s\u001b[0K\u001b[1G164.5 MiB [] 25% 1.9s\u001b[0K\u001b[1G164.5 MiB [] 26% 1.8s\u001b[0K\u001b[1G164.5 MiB [] 27% 1.8s\u001b[0K\u001b[1G164.5 MiB [] 28% 1.8s\u001b[0K\u001b[1G164.5 MiB [] 29% 1.8s\u001b[0K\u001b[1G164.5 MiB [] 29% 1.7s\u001b[0K\u001b[1G164.5 MiB [] 30% 1.7s\u001b[0K\u001b[1G164.5 MiB [] 31% 1.6s\u001b[0K\u001b[1G164.5 MiB [] 32% 1.6s\u001b[0K\u001b[1G164.5 MiB [] 33% 1.6s\u001b[0K\u001b[1G164.5 MiB [] 34% 1.5s\u001b[0K\u001b[1G164.5 MiB [] 35% 1.5s\u001b[0K\u001b[1G164.5 MiB [] 36% 1.5s\u001b[0K\u001b[1G164.5 MiB [] 38% 1.4s\u001b[0K\u001b[1G164.5 MiB [] 39% 1.4s\u001b[0K\u001b[1G164.5 MiB [] 40% 1.3s\u001b[0K\u001b[1G164.5 MiB [] 41% 1.3s\u001b[0K\u001b[1G164.5 MiB [] 42% 1.2s\u001b[0K\u001b[1G164.5 MiB [] 43% 1.2s\u001b[0K\u001b[1G164.5 MiB [] 45% 1.2s\u001b[0K\u001b[1G164.5 MiB [] 46% 1.1s\u001b[0K\u001b[1G164.5 MiB [] 47% 1.1s\u001b[0K\u001b[1G164.5 MiB [] 48% 1.1s\u001b[0K\u001b[1G164.5 MiB [] 49% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 50% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 51% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 52% 1.0s\u001b[0K\u001b[1G164.5 MiB [] 53% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 54% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 55% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 56% 0.9s\u001b[0K\u001b[1G164.5 MiB [] 58% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 59% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 60% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 61% 0.8s\u001b[0K\u001b[1G164.5 MiB [] 62% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 64% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 65% 0.7s\u001b[0K\u001b[1G164.5 MiB [] 66% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 67% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 68% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 69% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 70% 0.6s\u001b[0K\u001b[1G164.5 MiB [] 71% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 72% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 73% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 74% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 75% 0.5s\u001b[0K\u001b[1G164.5 MiB [] 76% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 77% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 78% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 79% 0.4s\u001b[0K\u001b[1G164.5 MiB [] 81% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 82% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 83% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 84% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 85% 0.3s\u001b[0K\u001b[1G164.5 MiB [] 87% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 88% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 89% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 90% 0.2s\u001b[0K\u001b[1G164.5 MiB [] 92% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 94% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 96% 0.1s\u001b[0K\u001b[1G164.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G164.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 130.0.6723.31 (playwright build v1140) downloaded to /root/.cache/ms-playwright/chromium-1140\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 6% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 29% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 75% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Downloading Firefox 131.0 (playwright build v1465)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1465/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G86.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G86.7 MiB [] 0% 2.5s\u001b[0K\u001b[1G86.7 MiB [] 1% 2.8s\u001b[0K\u001b[1G86.7 MiB [] 1% 3.6s\u001b[0K\u001b[1G86.7 MiB [] 1% 3.5s\u001b[0K\u001b[1G86.7 MiB [] 2% 2.7s\u001b[0K\u001b[1G86.7 MiB [] 3% 2.4s\u001b[0K\u001b[1G86.7 MiB [] 4% 2.2s\u001b[0K\u001b[1G86.7 MiB [] 5% 2.1s\u001b[0K\u001b[1G86.7 MiB [] 6% 2.1s\u001b[0K\u001b[1G86.7 MiB [] 7% 2.1s\u001b[0K\u001b[1G86.7 MiB [] 8% 2.0s\u001b[0K\u001b[1G86.7 MiB [] 9% 1.9s\u001b[0K\u001b[1G86.7 MiB [] 11% 1.7s\u001b[0K\u001b[1G86.7 MiB [] 12% 1.6s\u001b[0K\u001b[1G86.7 MiB [] 14% 1.5s\u001b[0K\u001b[1G86.7 MiB [] 15% 1.4s\u001b[0K\u001b[1G86.7 MiB [] 17% 1.4s\u001b[0K\u001b[1G86.7 MiB [] 18% 1.3s\u001b[0K\u001b[1G86.7 MiB [] 19% 1.3s\u001b[0K\u001b[1G86.7 MiB [] 20% 1.3s\u001b[0K\u001b[1G86.7 MiB [] 20% 1.5s\u001b[0K\u001b[1G86.7 MiB [] 20% 1.6s\u001b[0K\u001b[1G86.7 MiB [] 21% 1.7s\u001b[0K\u001b[1G86.7 MiB [] 22% 1.7s\u001b[0K\u001b[1G86.7 MiB [] 23% 1.6s\u001b[0K\u001b[1G86.7 MiB [] 24% 1.6s\u001b[0K\u001b[1G86.7 MiB [] 26% 1.5s\u001b[0K\u001b[1G86.7 MiB [] 28% 1.4s\u001b[0K\u001b[1G86.7 MiB [] 29% 1.3s\u001b[0K\u001b[1G86.7 MiB [] 30% 1.3s\u001b[0K\u001b[1G86.7 MiB [] 31% 1.3s\u001b[0K\u001b[1G86.7 MiB [] 33% 1.2s\u001b[0K\u001b[1G86.7 MiB [] 35% 1.2s\u001b[0K\u001b[1G86.7 MiB [] 37% 1.1s\u001b[0K\u001b[1G86.7 MiB [] 38% 1.1s\u001b[0K\u001b[1G86.7 MiB [] 39% 1.0s\u001b[0K\u001b[1G86.7 MiB [] 41% 1.0s\u001b[0K\u001b[1G86.7 MiB [] 43% 0.9s\u001b[0K\u001b[1G86.7 MiB [] 45% 0.9s\u001b[0K\u001b[1G86.7 MiB [] 46% 0.8s\u001b[0K\u001b[1G86.7 MiB [] 48% 0.8s\u001b[0K\u001b[1G86.7 MiB [] 49% 0.8s\u001b[0K\u001b[1G86.7 MiB [] 51% 0.7s\u001b[0K\u001b[1G86.7 MiB [] 53% 0.7s\u001b[0K\u001b[1G86.7 MiB [] 55% 0.7s\u001b[0K\u001b[1G86.7 MiB [] 57% 0.6s\u001b[0K\u001b[1G86.7 MiB [] 59% 0.6s\u001b[0K\u001b[1G86.7 MiB [] 60% 0.6s\u001b[0K\u001b[1G86.7 MiB [] 62% 0.5s\u001b[0K\u001b[1G86.7 MiB [] 64% 0.5s\u001b[0K\u001b[1G86.7 MiB [] 65% 0.5s\u001b[0K\u001b[1G86.7 MiB [] 67% 0.4s\u001b[0K\u001b[1G86.7 MiB [] 69% 0.4s\u001b[0K\u001b[1G86.7 MiB [] 70% 0.4s\u001b[0K\u001b[1G86.7 MiB [] 72% 0.4s\u001b[0K\u001b[1G86.7 MiB [] 74% 0.3s\u001b[0K\u001b[1G86.7 MiB [] 76% 0.3s\u001b[0K\u001b[1G86.7 MiB [] 79% 0.3s\u001b[0K\u001b[1G86.7 MiB [] 81% 0.2s\u001b[0K\u001b[1G86.7 MiB [] 83% 0.2s\u001b[0K\u001b[1G86.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G86.7 MiB [] 86% 0.2s\u001b[0K\u001b[1G86.7 MiB [] 89% 0.1s\u001b[0K\u001b[1G86.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G86.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G86.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G86.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G86.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G86.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 131.0 (playwright build v1465) downloaded to /root/.cache/ms-playwright/firefox-1465\n",
            "Downloading Webkit 18.0 (playwright build v2083)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2083/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G90.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G90.5 MiB [] 0% 19.1s\u001b[0K\u001b[1G90.5 MiB [] 0% 25.8s\u001b[0K\u001b[1G90.5 MiB [] 0% 31.1s\u001b[0K\u001b[1G90.5 MiB [] 0% 35.2s\u001b[0K\u001b[1G90.5 MiB [] 0% 47.9s\u001b[0K\u001b[1G90.5 MiB [] 0% 82.7s\u001b[0K\u001b[1G90.5 MiB [] 0% 117.5s\u001b[0K\u001b[1G90.5 MiB [] 0% 122.2s\u001b[0K\u001b[1G90.5 MiB [] 0% 119.6s\u001b[0K\u001b[1G90.5 MiB [] 0% 117.1s\u001b[0K\u001b[1G90.5 MiB [] 0% 115.5s\u001b[0K\u001b[1G90.5 MiB [] 0% 105.9s\u001b[0K\u001b[1G90.5 MiB [] 0% 103.6s\u001b[0K\u001b[1G90.5 MiB [] 0% 95.5s\u001b[0K\u001b[1G90.5 MiB [] 0% 90.6s\u001b[0K\u001b[1G90.5 MiB [] 0% 88.1s\u001b[0K\u001b[1G90.5 MiB [] 0% 79.8s\u001b[0K\u001b[1G90.5 MiB [] 1% 59.5s\u001b[0K\u001b[1G90.5 MiB [] 1% 48.2s\u001b[0K\u001b[1G90.5 MiB [] 1% 47.1s\u001b[0K\u001b[1G90.5 MiB [] 1% 44.5s\u001b[0K\u001b[1G90.5 MiB [] 1% 43.8s\u001b[0K\u001b[1G90.5 MiB [] 1% 44.0s\u001b[0K\u001b[1G90.5 MiB [] 1% 45.4s\u001b[0K\u001b[1G90.5 MiB [] 2% 40.8s\u001b[0K\u001b[1G90.5 MiB [] 2% 38.4s\u001b[0K\u001b[1G90.5 MiB [] 2% 37.0s\u001b[0K\u001b[1G90.5 MiB [] 2% 36.6s\u001b[0K\u001b[1G90.5 MiB [] 2% 36.2s\u001b[0K\u001b[1G90.5 MiB [] 2% 35.4s\u001b[0K\u001b[1G90.5 MiB [] 2% 35.0s\u001b[0K\u001b[1G90.5 MiB [] 3% 29.0s\u001b[0K\u001b[1G90.5 MiB [] 4% 22.4s\u001b[0K\u001b[1G90.5 MiB [] 5% 18.8s\u001b[0K\u001b[1G90.5 MiB [] 5% 18.1s\u001b[0K\u001b[1G90.5 MiB [] 5% 17.6s\u001b[0K\u001b[1G90.5 MiB [] 5% 17.1s\u001b[0K\u001b[1G90.5 MiB [] 6% 16.3s\u001b[0K\u001b[1G90.5 MiB [] 6% 15.7s\u001b[0K\u001b[1G90.5 MiB [] 7% 15.1s\u001b[0K\u001b[1G90.5 MiB [] 7% 14.9s\u001b[0K\u001b[1G90.5 MiB [] 7% 14.3s\u001b[0K\u001b[1G90.5 MiB [] 7% 14.4s\u001b[0K\u001b[1G90.5 MiB [] 7% 14.2s\u001b[0K\u001b[1G90.5 MiB [] 8% 13.6s\u001b[0K\u001b[1G90.5 MiB [] 8% 13.3s\u001b[0K\u001b[1G90.5 MiB [] 8% 13.2s\u001b[0K\u001b[1G90.5 MiB [] 9% 13.1s\u001b[0K\u001b[1G90.5 MiB [] 9% 12.9s\u001b[0K\u001b[1G90.5 MiB [] 9% 12.6s\u001b[0K\u001b[1G90.5 MiB [] 9% 12.3s\u001b[0K\u001b[1G90.5 MiB [] 10% 11.8s\u001b[0K\u001b[1G90.5 MiB [] 10% 11.3s\u001b[0K\u001b[1G90.5 MiB [] 11% 10.5s\u001b[0K\u001b[1G90.5 MiB [] 12% 9.6s\u001b[0K\u001b[1G90.5 MiB [] 13% 9.1s\u001b[0K\u001b[1G90.5 MiB [] 14% 9.1s\u001b[0K\u001b[1G90.5 MiB [] 17% 7.1s\u001b[0K\u001b[1G90.5 MiB [] 18% 6.9s\u001b[0K\u001b[1G90.5 MiB [] 18% 6.6s\u001b[0K\u001b[1G90.5 MiB [] 19% 6.4s\u001b[0K\u001b[1G90.5 MiB [] 20% 6.1s\u001b[0K\u001b[1G90.5 MiB [] 21% 5.8s\u001b[0K\u001b[1G90.5 MiB [] 22% 5.5s\u001b[0K\u001b[1G90.5 MiB [] 23% 5.3s\u001b[0K\u001b[1G90.5 MiB [] 24% 5.0s\u001b[0K\u001b[1G90.5 MiB [] 25% 4.8s\u001b[0K\u001b[1G90.5 MiB [] 26% 4.5s\u001b[0K\u001b[1G90.5 MiB [] 28% 4.3s\u001b[0K\u001b[1G90.5 MiB [] 28% 4.2s\u001b[0K\u001b[1G90.5 MiB [] 29% 4.2s\u001b[0K\u001b[1G90.5 MiB [] 29% 4.1s\u001b[0K\u001b[1G90.5 MiB [] 30% 4.0s\u001b[0K\u001b[1G90.5 MiB [] 31% 3.9s\u001b[0K\u001b[1G90.5 MiB [] 32% 3.8s\u001b[0K\u001b[1G90.5 MiB [] 32% 3.7s\u001b[0K\u001b[1G90.5 MiB [] 33% 3.6s\u001b[0K\u001b[1G90.5 MiB [] 34% 3.5s\u001b[0K\u001b[1G90.5 MiB [] 35% 3.4s\u001b[0K\u001b[1G90.5 MiB [] 36% 3.3s\u001b[0K\u001b[1G90.5 MiB [] 38% 3.0s\u001b[0K\u001b[1G90.5 MiB [] 39% 3.0s\u001b[0K\u001b[1G90.5 MiB [] 40% 3.0s\u001b[0K\u001b[1G90.5 MiB [] 40% 2.9s\u001b[0K\u001b[1G90.5 MiB [] 41% 2.9s\u001b[0K\u001b[1G90.5 MiB [] 42% 2.9s\u001b[0K\u001b[1G90.5 MiB [] 43% 2.9s\u001b[0K\u001b[1G90.5 MiB [] 44% 2.8s\u001b[0K\u001b[1G90.5 MiB [] 46% 2.7s\u001b[0K\u001b[1G90.5 MiB [] 46% 2.6s\u001b[0K\u001b[1G90.5 MiB [] 47% 2.6s\u001b[0K\u001b[1G90.5 MiB [] 49% 2.5s\u001b[0K\u001b[1G90.5 MiB [] 50% 2.3s\u001b[0K\u001b[1G90.5 MiB [] 51% 2.2s\u001b[0K\u001b[1G90.5 MiB [] 53% 2.1s\u001b[0K\u001b[1G90.5 MiB [] 54% 2.0s\u001b[0K\u001b[1G90.5 MiB [] 55% 1.9s\u001b[0K\u001b[1G90.5 MiB [] 56% 1.9s\u001b[0K\u001b[1G90.5 MiB [] 57% 1.8s\u001b[0K\u001b[1G90.5 MiB [] 58% 1.7s\u001b[0K\u001b[1G90.5 MiB [] 60% 1.7s\u001b[0K\u001b[1G90.5 MiB [] 61% 1.6s\u001b[0K\u001b[1G90.5 MiB [] 62% 1.5s\u001b[0K\u001b[1G90.5 MiB [] 64% 1.4s\u001b[0K\u001b[1G90.5 MiB [] 66% 1.3s\u001b[0K\u001b[1G90.5 MiB [] 68% 1.2s\u001b[0K\u001b[1G90.5 MiB [] 69% 1.1s\u001b[0K\u001b[1G90.5 MiB [] 71% 1.1s\u001b[0K\u001b[1G90.5 MiB [] 72% 1.0s\u001b[0K\u001b[1G90.5 MiB [] 74% 0.9s\u001b[0K\u001b[1G90.5 MiB [] 75% 0.9s\u001b[0K\u001b[1G90.5 MiB [] 77% 0.8s\u001b[0K\u001b[1G90.5 MiB [] 78% 0.7s\u001b[0K\u001b[1G90.5 MiB [] 79% 0.7s\u001b[0K\u001b[1G90.5 MiB [] 81% 0.6s\u001b[0K\u001b[1G90.5 MiB [] 83% 0.6s\u001b[0K\u001b[1G90.5 MiB [] 84% 0.5s\u001b[0K\u001b[1G90.5 MiB [] 85% 0.5s\u001b[0K\u001b[1G90.5 MiB [] 87% 0.4s\u001b[0K\u001b[1G90.5 MiB [] 88% 0.4s\u001b[0K\u001b[1G90.5 MiB [] 89% 0.3s\u001b[0K\u001b[1G90.5 MiB [] 91% 0.3s\u001b[0K\u001b[1G90.5 MiB [] 92% 0.2s\u001b[0K\u001b[1G90.5 MiB [] 93% 0.2s\u001b[0K\u001b[1G90.5 MiB [] 94% 0.2s\u001b[0K\u001b[1G90.5 MiB [] 96% 0.1s\u001b[0K\u001b[1G90.5 MiB [] 97% 0.1s\u001b[0K\u001b[1G90.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G90.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.0 (playwright build v2083) downloaded to /root/.cache/ms-playwright/webkit-2083\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:707:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:805:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/server/registry/index.js:794:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.10/dist-packages/playwright/driver/package/lib/cli/program.js:119:7)\n"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain-openai langchain playwright beautifulsoup4\n",
        "!playwright install\n",
        "\n",
        "# Set env var OPENAI_API_KEY or load from a .env file:\n",
        "# import dotenv\n",
        "# dotenv.load_dotenv()\n",
        "# import dotenv\n",
        "# dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50741083",
      "metadata": {
        "id": "50741083"
      },
      "source": [
        "Scraping HTML content using a headless instance of Chromium.\n",
        "\n",
        "* The async nature of the scraping process is handled using Python's asyncio library.\n",
        "* The actual interaction with the web pages is handled by Playwright."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cd457cb1",
      "metadata": {
        "id": "cd457cb1"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain-community\n",
        "\n",
        "# from langchain_community.document_loaders import AsyncChromiumLoader\n",
        "# from langchain_community.document_transformers import BeautifulSoupTransformer\n",
        "\n",
        "# # Load HTML\n",
        "# loader = AsyncChromiumLoader([\"https://www.wsj.com\"])\n",
        "# html = await loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "\n",
        "from langchain_community.document_loaders import AsyncChromiumLoader\n",
        "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
        "import asyncio\n",
        "\n",
        "async def load_html():\n",
        "    # Load HTML\n",
        "    loader = AsyncChromiumLoader([\"https://www.wsj.com\"])\n",
        "    # Create a task to run the loader's asynchronous load method\n",
        "    task = asyncio.create_task(loader.aload())\n",
        "    # Await the task to get the result\n",
        "    html = await task\n",
        "    return html\n",
        "\n",
        "\n",
        "# Run the async function to load the HTML\n",
        "html = asyncio.run(load_html())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLel7QCuiRXG",
        "outputId": "a2ef4d39-8845-469f-c23a-5116dd4699be"
      },
      "id": "nLel7QCuiRXG",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.7)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.15)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.139)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a879806",
      "metadata": {
        "id": "2a879806"
      },
      "source": [
        "Scrape text content tags such as `<p>, <li>, <div>, and <a>` tags from the HTML content:\n",
        "\n",
        "* `<p>`: The paragraph tag. It defines a paragraph in HTML and is used to group together related sentences and/or phrases.\n",
        "\n",
        "* `<li>`: The list item tag. It is used within ordered (`<ol>`) and unordered (`<ul>`) lists to define individual items within the list.\n",
        "\n",
        "* `<div>`: The division tag. It is a block-level element used to group other inline or block-level elements.\n",
        "\n",
        "* `<a>`: The anchor tag. It is used to define hyperlinks.\n",
        "\n",
        "* `<span>`:  an inline container used to mark up a part of a text, or a part of a document.\n",
        "\n",
        "For many news websites (e.g., WSJ, CNN), headlines and summaries are all in `<span>` tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "141f206b",
      "metadata": {
        "id": "141f206b"
      },
      "outputs": [],
      "source": [
        "# Transform\n",
        "bs_transformer = BeautifulSoupTransformer()\n",
        "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "73ddb234",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "73ddb234",
        "outputId": "aa46c393-35bf-4440-d6f1-ec762b4cdfdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Result\n",
        "docs_transformed[0].page_content[0:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d26d185",
      "metadata": {
        "id": "7d26d185"
      },
      "source": [
        "These `Documents` now are staged for downstream usage in various LLM apps, as discussed below.\n",
        "\n",
        "## Loader\n",
        "\n",
        "### AsyncHtmlLoader\n",
        "\n",
        "The [AsyncHtmlLoader](/docs/integrations/document_loaders/async_html) uses the `aiohttp` library to make asynchronous HTTP requests, suitable for simpler and lightweight scraping.\n",
        "\n",
        "### AsyncChromiumLoader\n",
        "\n",
        "The [AsyncChromiumLoader](/docs/integrations/document_loaders/async_chromium) uses Playwright to launch a Chromium instance, which can handle JavaScript rendering and more complex web interactions.\n",
        "\n",
        "Chromium is one of the browsers supported by Playwright, a library used to control browser automation.\n",
        "\n",
        "Headless mode means that the browser is running without a graphical user interface, which is commonly used for web scraping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8941e855",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8941e855",
        "outputId": "84aa3cae-0708-492b-af4b-27aacb528b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Fetching pages: 100%|##########| 2/2 [00:00<00:00, 32.11it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/bs4/element.py:175: RuntimeWarning: coroutine 'AsyncChromiumLoader.ascrape_playwright' was never awaited\n",
            "  self.parent = parent\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/usr/local/lib/python3.10/dist-packages/bs4/element.py:175: RuntimeWarning: coroutine 'load_html' was never awaited\n",
            "  self.parent = parent\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import AsyncHtmlLoader\n",
        "\n",
        "urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
        "loader = AsyncHtmlLoader(urls)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e47f4bf0",
      "metadata": {
        "id": "e47f4bf0"
      },
      "source": [
        "## Transformer\n",
        "\n",
        "### HTML2Text\n",
        "\n",
        "[HTML2Text](/docs/integrations/document_transformers/html2text) provides a straightforward conversion of HTML content into plain text (with markdown-like formatting) without any specific tag manipulation.\n",
        "\n",
        "It's best suited for scenarios where the goal is to extract human-readable text without needing to manipulate specific HTML elements.\n",
        "\n",
        "### Beautiful Soup\n",
        "\n",
        "Beautiful Soup offers more fine-grained control over HTML content, enabling specific tag extraction, removal, and content cleaning.\n",
        "\n",
        "It's suited for cases where you want to extract specific information and clean up the HTML content according to your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "99a7e2a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99a7e2a8",
        "outputId": "50678160-a985-48f7-d3df-b040636c082d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching pages: 100%|##########| 2/2 [00:00<00:00, 61.61it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import AsyncHtmlLoader\n",
        "\n",
        "urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
        "loader = AsyncHtmlLoader(urls)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a2cd3e8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "a2cd3e8d",
        "outputId": "5b93597a-e2a3-4559-9d8f-6ce486295eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting html2text\n",
            "  Downloading html2text-2024.2.26.tar.gz (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: html2text\n",
            "  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html2text: filename=html2text-2024.2.26-py3-none-any.whl size=33111 sha256=4320a294aa98bd077bfb332a5c67caaa2dcef1943aed6850ec1de4f05e6d08e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/96/6d/a7eba8f80d31cbd188a2787b81514d82fc5ae6943c44777659\n",
            "Successfully built html2text\n",
            "Installing collected packages: html2text\n",
            "Successfully installed html2text-2024.2.26\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Skip to main content  Skip to navigation\\n\\n<\\n\\n>\\n\\nMenu\\n\\n## ESPN\\n\\n  *   *   *   * scores\\n\\nNEW! Find where to watch all of your favorite sports!\\n\\n  * NFL\\n  * NBA\\n  * MLB\\n  * NCAAF\\n  * NHL\\n  * Soccer\\n  * MMA\\n  * More Sports\\n\\n    * Boxing\\n    * CFL\\n    * NCAA\\n    * Cricket\\n    * F1\\n    * Golf\\n    * Horse\\n    * LLWS\\n    * NASCAR\\n    * NBA G League\\n    * NBA Summer League\\n    * NCAAM\\n    * NCAAW\\n    * NWSL\\n    * Olympics\\n    * PLL\\n    * Professional Wrestling\\n    * Racing\\n    * RN BB\\n    * RN FB\\n    * R'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "!pip install html2text\n",
        "from langchain_community.document_transformers import Html2TextTransformer\n",
        "\n",
        "html2text = Html2TextTransformer()\n",
        "docs_transformed = html2text.transform_documents(docs)\n",
        "docs_transformed[0].page_content[0:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aef9861",
      "metadata": {
        "id": "8aef9861"
      },
      "source": [
        "## Scraping with extraction\n",
        "\n",
        "### LLM with function calling\n",
        "\n",
        "Web scraping is challenging for many reasons.\n",
        "\n",
        "One of them is the changing nature of modern websites' layouts and content, which requires modifying scraping scripts to accommodate the changes.\n",
        "\n",
        "Using Function (e.g., OpenAI) with an extraction chain, we avoid having to change your code constantly when websites change.\n",
        "\n",
        "We're using `gpt-3.5-turbo-0613` to guarantee access to OpenAI Functions feature (although this might be available to everyone by time of writing).\n",
        "\n",
        "We're also keeping `temperature` at `0` to keep randomness of the LLM down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "52d49f6f",
      "metadata": {
        "id": "52d49f6f"
      },
      "outputs": [],
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install google-api-python-client\n",
        "\n",
        "# from googleapiclient.discovery import build\n",
        "\n",
        "# # Replace 'YOUR_API_KEY' with your actual Google API key\n",
        "# service = build(\"customsearch\", \"v1\", developerKey=\"AIzaSyASVv88T71KH3DO9LuDOkZ5SzX7LP-mjt8\")"
      ],
      "metadata": {
        "id": "hFS65SlAudfd"
      },
      "id": "hFS65SlAudfd",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\"\n"
      ],
      "metadata": {
        "id": "Xz6h4dvgvL0S"
      },
      "id": "Xz6h4dvgvL0S",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import google.generativeai as genai\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "CZIrG-r5vUGj"
      },
      "id": "CZIrG-r5vUGj",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = genai.GenerativeModel('gemini-1.5-flash')"
      ],
      "metadata": {
        "id": "A590abc8vaIu"
      },
      "id": "A590abc8vaIu",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fc5757ce",
      "metadata": {
        "id": "fc5757ce"
      },
      "source": [
        "### Define a schema\n",
        "\n",
        "Next, you define a schema to specify what kind of data you want to extract.\n",
        "\n",
        "Here, the key names matter as they tell the LLM what kind of information they want.\n",
        "\n",
        "So, be as detailed as possible.\n",
        "\n",
        "In this example, we want to scrape only news article's name and summary from The Wall Street Journal website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "95506f8e",
      "metadata": {
        "id": "95506f8e"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_extraction_chain\n",
        "\n",
        "schema = {\n",
        "    \"properties\": {\n",
        "        \"news_article_title\": {\"type\": \"string\"},\n",
        "        \"news_article_summary\": {\"type\": \"string\"},\n",
        "    },\n",
        "    \"required\": [\"news_article_title\", \"news_article_summary\"],\n",
        "}\n",
        "\n",
        "\n",
        "def extract(content: str, schema: dict):\n",
        "    return create_extraction_chain(schema=schema, llm=llm).run(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f7de42",
      "metadata": {
        "id": "97f7de42"
      },
      "source": [
        "### Run the web scraper w/ BeautifulSoup\n",
        "\n",
        "As shown above, we'll be using `BeautifulSoupTransformer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "977560ba",
      "metadata": {
        "id": "977560ba"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "def scrape_with_playwright(urls, schema):\n",
        "    loader = AsyncChromiumLoader(urls)\n",
        "    docs = loader.load()\n",
        "    bs_transformer = BeautifulSoupTransformer()\n",
        "    docs_transformed = bs_transformer.transform_documents(\n",
        "        docs, tags_to_extract=[\"span\"]\n",
        "    )\n",
        "    print(\"Extracting content with LLM\")\n",
        "\n",
        "    # Grab the first 1000 tokens of the site\n",
        "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=1000, chunk_overlap=0\n",
        "    )\n",
        "    splits = splitter.split_documents(docs_transformed)\n",
        "\n",
        "    # Process the first split\n",
        "    extracted_content = extract(schema=schema, content=splits[0].page_content)\n",
        "    pprint.pprint(extracted_content)\n",
        "    return extracted_content\n",
        "\n",
        "\n",
        "urls = [\"https://www.wsj.com\"]\n",
        "extracted_content = scrape_with_playwright(urls, schema=schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08a8cef",
      "metadata": {
        "id": "b08a8cef"
      },
      "source": [
        "We can compare the headlines scraped to the page:\n",
        "\n",
        "![Image description](https://github.com/langchain-ai/langchain/blob/v0.1/docs/static/img/wsj_page.png?raw=1)\n",
        "\n",
        "Looking at the [LangSmith trace](https://smith.langchain.com/public/c3070198-5b13-419b-87bf-3821cdf34fa6/r), we can see what is going on under the hood:\n",
        "\n",
        "* It's following what is explained in the [extraction](docs/use_cases/extraction).\n",
        "* We call the `information_extraction` function on the input text.\n",
        "* It will attempt to populate the provided schema from the url content."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a6f11e",
      "metadata": {
        "id": "a5a6f11e"
      },
      "source": [
        "## Research automation\n",
        "\n",
        "Related to scraping, we may want to answer specific questions using searched content.\n",
        "\n",
        "We can automate the process of [web research](https://blog.langchain.dev/automating-web-research/) using a retriever, such as the `WebResearchRetriever`.\n",
        "\n",
        "![Image description](https://github.com/langchain-ai/langchain/blob/v0.1/docs/static/img/web_research.png?raw=1)\n",
        "\n",
        "Copy requirements [from here](https://github.com/langchain-ai/web-explorer/blob/main/requirements.txt):\n",
        "\n",
        "`pip install -r requirements.txt`\n",
        "\n",
        "Set `GOOGLE_CSE_ID` and `GOOGLE_API_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414f0d41",
      "metadata": {
        "id": "414f0d41"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.web_research import WebResearchRetriever\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d1ce098",
      "metadata": {
        "id": "5d1ce098"
      },
      "outputs": [],
      "source": [
        "# Vectorstore\n",
        "vectorstore = Chroma(\n",
        "    embedding_function=OpenAIEmbeddings(), persist_directory=\"./chroma_db_oai\"\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Search\n",
        "search = GoogleSearchAPIWrapper()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d808b9d",
      "metadata": {
        "id": "6d808b9d"
      },
      "source": [
        "Initialize retriever with the above tools to:\n",
        "    \n",
        "* Use an LLM to generate multiple relevant search queries (one LLM call)\n",
        "* Execute a search for each query\n",
        "* Choose the top K links per query  (multiple search calls in parallel)\n",
        "* Load the information from all chosen links (scrape pages in parallel)\n",
        "* Index those documents into a vectorstore\n",
        "* Find the most relevant documents for each original generated search query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e3a589",
      "metadata": {
        "id": "e3e3a589"
      },
      "outputs": [],
      "source": [
        "# Initialize\n",
        "web_research_retriever = WebResearchRetriever.from_llm(\n",
        "    vectorstore=vectorstore, llm=llm, search=search\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20655b74",
      "metadata": {
        "id": "20655b74"
      },
      "outputs": [],
      "source": [
        "# Run\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
        "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm, retriever=web_research_retriever\n",
        ")\n",
        "result = qa_chain({\"question\": user_input})\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a940df1",
      "metadata": {
        "id": "7a940df1"
      },
      "source": [
        "### Going deeper\n",
        "\n",
        "* Here's a [app](https://github.com/langchain-ai/web-explorer/tree/main) that wraps this retriever with a lighweight UI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "312c399e",
      "metadata": {
        "id": "312c399e"
      },
      "source": [
        "## Question answering over a website\n",
        "\n",
        "To answer questions over a specific website, you can use Apify's [Website Content Crawler](https://apify.com/apify/website-content-crawler) Actor, which can deeply crawl websites such as documentation, knowledge bases, help centers, or blogs,\n",
        "and extract text content from the web pages.\n",
        "\n",
        "In the example below, we will deeply crawl the Python documentation of LangChain's Chat LLM models and answer a question over it.\n",
        "\n",
        "First, install the requirements\n",
        "`pip install apify-client langchain-openai langchain`\n",
        "\n",
        "Next, set `OPENAI_API_KEY` and `APIFY_API_TOKEN` in your environment variables.\n",
        "\n",
        "The full code follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b08da5e",
      "metadata": {
        "id": "9b08da5e"
      },
      "outputs": [],
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_community.docstore.document import Document\n",
        "from langchain_community.utilities import ApifyWrapper\n",
        "\n",
        "apify = ApifyWrapper()\n",
        "# Call the Actor to obtain text from the crawled webpages\n",
        "loader = apify.call_actor(\n",
        "    actor_id=\"apify/website-content-crawler\",\n",
        "    run_input={\"startUrls\": [{\"url\": \"/docs/integrations/chat/\"}]},\n",
        "    dataset_mapping_function=lambda item: Document(\n",
        "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Create a vector store based on the crawled data\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "# Query the vector store\n",
        "query = \"Are any OpenAI chat models integrated in LangChain?\"\n",
        "result = index.query(query)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}